---
title: >-
  Paying More Attention to Attention: Improving the Performance of Convolutional
  Neural Networks via Attention Transfer
date: 2019-01-25 09:35:59
categories:
- CV
tags:
- Knowledge Distillation
- Attention
---

We propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures.

<!-- more -->

# INTRODUCTION

One of the popular hypothesis there is that there are non-attentional and attentional perception processes. Non-attentional processes help to observe a scene in general and gather high-level information, which, when associated with other thinking processes, helps us to control the attention processes and navigate to a certain part of the scene.

![attention_transfer](transfer_attention.PNG)